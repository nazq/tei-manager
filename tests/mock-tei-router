#!/usr/bin/env bash
# Mock text-embeddings-router for testing
# Simulates a simple TEI instance by running a basic HTTP server

PORT=8080
MODEL_ID=""

# Parse arguments
while [[ $# -gt 0 ]]; do
    case $1 in
        --port)
            PORT="$2"
            shift 2
            ;;
        --model-id)
            MODEL_ID="$2"
            shift 2
            ;;
        --max-batch-tokens|--max-concurrent-requests|--pooling)
            # Skip these args and their values
            shift 2
            ;;
        --json-output|*)
            # Skip flags without values
            shift
            ;;
    esac
done

echo "{\"level\":\"info\",\"message\":\"Mock TEI router starting\",\"port\":$PORT,\"model\":\"$MODEL_ID\"}"

# Simple HTTP server using uv run
exec uv run --no-project - <<EOF
import http.server
import socketserver
import json
import os
import urllib.request
import urllib.error

PORT = int(os.environ.get('PORT', $PORT))
MODEL_ID = os.environ.get('MODEL_ID', '$MODEL_ID')

# Known model dimensions (fallback)
MODEL_DIMS = {
    # Dense models
    'BAAI/bge-small-en-v1.5': 384,
    'BAAI/bge-base-en-v1.5': 768,
    'BAAI/bge-large-en-v1.5': 1024,
    'sentence-transformers/all-mpnet-base-v2': 768,
    'sentence-transformers/all-MiniLM-L6-v2': 384,
    'sentence-transformers/all-MiniLM-L12-v2': 384,
    'intfloat/e5-small-v2': 384,
    'intfloat/e5-base-v2': 768,
    'intfloat/e5-large-v2': 1024,
    # Sparse models (SPLADE) - vocabulary size
    'naver/splade-cocondenser-ensembledistil': 30522,
    'naver/splade_v2_max': 30522,
    'naver/splade-v3': 30522,
}

# Sparse models (SPLADE uses different output format)
SPARSE_MODELS = {
    'naver/splade-cocondenser-ensembledistil',
    'naver/splade_v2_max',
    'naver/splade-v3',
}

def get_model_dimension(model_id):
    """Fetch model dimension from HuggingFace config or use lookup table"""
    # Check lookup table first
    if model_id in MODEL_DIMS:
        return MODEL_DIMS[model_id]

    # Try fetching from HuggingFace
    try:
        config_url = f'https://huggingface.co/{model_id}/raw/main/config.json'
        with urllib.request.urlopen(config_url, timeout=5) as response:
            config = json.loads(response.read().decode())
            # Try common config keys for embedding dimension
            for key in ['hidden_size', 'd_model', 'embedding_size', 'dim']:
                if key in config:
                    return config[key]
    except (urllib.error.URLError, urllib.error.HTTPError, Exception):
        pass

    # Default fallback
    return 768

# Get dimension once at startup
EMBEDDING_DIM = get_model_dimension(MODEL_ID)

class MockTEIHandler(http.server.BaseHTTPRequestHandler):
    def do_GET(self):
        if self.path == '/health':
            self.send_response(200)
            self.send_header('Content-type', 'application/json')
            self.end_headers()
            self.wfile.write(json.dumps({'status': 'ok'}).encode())
        elif self.path == '/info':
            self.send_response(200)
            self.send_header('Content-type', 'application/json')
            self.end_headers()
            self.wfile.write(json.dumps({
                'model_id': MODEL_ID,
                'version': '1.8.3-mock',
                'model_type': 'embedding',
                'max_input_length': 512,
                'max_batch_tokens': 16384,
                'embedding_dimension': EMBEDDING_DIM
            }).encode())
        else:
            self.send_response(404)
            self.end_headers()

    def do_POST(self):
        if self.path == '/embed':
            content_length = int(self.headers['Content-Length'])
            post_data = self.rfile.read(content_length)
            request_data = json.loads(post_data.decode())

            # Extract inputs (can be string or list of strings)
            inputs = request_data.get('inputs', '')
            if isinstance(inputs, str):
                inputs = [inputs]

            # Generate embeddings based on model type
            if MODEL_ID in SPARSE_MODELS:
                # SPLADE sparse embeddings: list of {token_id: weight} dicts
                # Generate ~20-30 non-zero values per input
                import random
                embeddings = []
                for _ in inputs:
                    sparse_vec = {}
                    num_nonzero = random.randint(20, 30)
                    for _ in range(num_nonzero):
                        token_id = random.randint(0, EMBEDDING_DIM - 1)
                        sparse_vec[str(token_id)] = round(random.uniform(0.1, 2.5), 4)
                    embeddings.append(sparse_vec)
            else:
                # Dense embeddings: list of dense vectors
                embeddings = [[0.1] * EMBEDDING_DIM for _ in inputs]

            self.send_response(200)
            self.send_header('Content-type', 'application/json')
            self.end_headers()
            self.wfile.write(json.dumps(embeddings).encode())
        else:
            self.send_response(404)
            self.end_headers()

    def log_message(self, format, *args):
        pass  # Suppress logs

with socketserver.TCPServer(('', PORT), MockTEIHandler) as httpd:
    httpd.serve_forever()
EOF
